{"cells":[{"cell_type":"markdown","metadata":{"id":"u2U7TIRdKNGu"},"source":["# [Autoencoders](https://arxiv.org/abs/2201.03898)"]},{"cell_type":"markdown","metadata":{"id":"it5nmu_-c5-E"},"source":["CNN을 훈련할 때 발생하는 문제 중 하나는 많은 레이블이 지정된 데이터가 필요하다는 것인데 이미지 분류의 경우 이미지를 여러 클래스로 분리해야 하는데, 이는 수작업으로 수행\n","\n","하지만 원시(레이블이 없는) 데이터를 사용하여 CNN 특징 추출기를 훈련할 수 있는데, 이를 **자기 지도 학습**이라고 함\n","- 레이블 대신 훈련 이미지를 네트워크 입력과 출력으로 사용\n","- 자동 인코더의 주요 아이디어는 입력 이미지를 어떤 **잠재 공간**으로 변환하는 **인코더 네트워크**(일반적으로 더 작은 크기의 벡터)와 원본 이미지를 재구성하는 것이 목표인 **디코더 네트워크**를 갖는다는 것\n","\n","정확한 재구성을 위해 원본 이미지에서 최대한 많은 정보를 캡처하도록 자동 인코더를 학습시키기 때문에, 네트워크는 입력 이미지의 의미를 포착할 수 있는 최적의 **임베딩**을 찾으려고 함\n","\n","Let's create simplest autoencoder for MNIST"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:39.262403Z","iopub.status.busy":"2022-04-08T00:15:39.262149Z","iopub.status.idle":"2022-04-08T00:15:39.268143Z","shell.execute_reply":"2022-04-08T00:15:39.267475Z","shell.execute_reply.started":"2022-04-08T00:15:39.262376Z"},"id":"6n8fzzN-2T1h","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (2.2.2)\n","Requirement already satisfied: filelock in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch) (2024.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: torchvision in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (0.17.2)\n","Requirement already satisfied: numpy in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: torch==2.2.2 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torchvision) (2.2.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torchvision) (10.3.0)\n","Requirement already satisfied: filelock in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (4.11.0)\n","Requirement already satisfied: sympy in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (1.12)\n","Requirement already satisfied: networkx in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (3.3)\n","Requirement already satisfied: jinja2 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (3.1.4)\n","Requirement already satisfied: fsspec in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (2024.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from jinja2->torch==2.2.2->torchvision) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from sympy->torch==2.2.2->torchvision) (1.3.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: matplotlib in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (3.8.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.21 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib) (24.0)\n","Requirement already satisfied: pillow>=8 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib) (10.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# 파이토치(PyTorch)와 관련된 라이브러리들을 임포트하고, 난수 생성을 위한 시드를 설정\n","\n","%pip install torch\n","%pip install torchvision\n","%pip install matplotlib\n","\n","# 필요한 모듈과 라이브러리를 임포트\n","# 딥러닝 모델을 구축하고, 훈련하며, 결과를 시각화하는 데 필요\n","import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","from torchvision import transforms\n","from torch import nn\n","from torch import optim\n","from tqdm import tqdm\n","import numpy as np\n","import torch.nn.functional as F\n","\n","# PyTorch와 NumPy의 난수 생성기에 시드 값을 설정\n","# 코드 실행 결과의 일관성과 재현성을 보장\n","torch.manual_seed(42)\n","np.random.seed(42)"]},{"cell_type":"markdown","metadata":{},"source":["학습 매개변수를 정의하고 GPU를 사용할 수 있는지 확인"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:39.530041Z","iopub.status.busy":"2022-04-08T00:15:39.529522Z","iopub.status.idle":"2022-04-08T00:15:39.534247Z","shell.execute_reply":"2022-04-08T00:15:39.533327Z","shell.execute_reply.started":"2022-04-08T00:15:39.530006Z"},"id":"bjL-jOgi3gmG","trusted":true},"outputs":[],"source":["device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","train_size = 0.9 # 데이터 세트의 90%를 훈련 데이터로 사용하고 나머지 10%는 검증 또는 테스트 용도로 사용하기 위해 설정된 비율\n","lr = 1e-3\n","eps = 1e-8 # 수치 계산 시 0으로 나누는 것을 방지하기 위해 사용되는 매우 작은 값을 최적화 알고리즘에서 안정성을 높이기 위해 사용\n","batch_size = 256\n","epochs = 30"]},{"cell_type":"markdown","metadata":{},"source":["MNIST 데이터 세트를 로드하고 지정된 변환을 적용합니다. 또한 이를 훈련/테스트 데이터 세트로 분할"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:39.893727Z","iopub.status.busy":"2022-04-08T00:15:39.893160Z","iopub.status.idle":"2022-04-08T00:15:39.899407Z","shell.execute_reply":"2022-04-08T00:15:39.898565Z","shell.execute_reply.started":"2022-04-08T00:15:39.893688Z"},"id":"g2Eo43713Sxb","trusted":true},"outputs":[],"source":["def mnist(train_part, transform=None):\n","    dataset = torchvision.datasets.MNIST('.', download=True, transform=transform)\n","    # 함수 인자로 받은 train_part는 훈련 데이터셋의 비율을 결정\n","    # 예를 들어, 0.9를 인자로 주면 데이터의 90%를 훈련용으로 사용\n","    train_part = int(train_part * len(dataset))\n","    # 전체 데이터셋에서 train_part 비율만큼을 훈련 데이터셋으로, 나머지를 테스트 데이터셋으로 분할\n","    # 이 분할은 torch.utils.data.random_split 함수를 사용하여 무작위로 수행\n","    # 분할 비율은 train_part 계산을 통해 결정\n","    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_part, len(dataset) - train_part])\n","    return train_dataset, test_dataset"]},{"cell_type":"markdown","metadata":{},"source":["데이터 세트를 로드하고 학습 및 테스트를 위한 데이터 로더를 정의"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:41.292476Z","iopub.status.busy":"2022-04-08T00:15:41.292238Z","iopub.status.idle":"2022-04-08T00:15:41.299801Z","shell.execute_reply":"2022-04-08T00:15:41.298897Z","shell.execute_reply.started":"2022-04-08T00:15:41.292449Z"},"id":"jAI3uK86_zHM","trusted":true},"outputs":[],"source":["# transforms.Compose를 사용해 데이터에 적용할 전처리 단계를 정의\n","# transforms.ToTensor()를 통해 이미지 데이터를 PyTorch 텐서로 변환하고, 픽셀 값의 범위를 [0, 255]에서 [0.0, 1.0]으로 조정\n","transform = transforms.Compose([transforms.ToTensor()])\n","\n","# transform과 함께 mnist 함수를 호출하여 훈련 데이터와 테스트 데이터를 분할\n","# train_size 변수를 사용하여 데이터의 어떤 비율을 훈련 데이터로 사용할지 결정\n","train_dataset, test_dataset = mnist(train_size, transform)\n","\n","# 훈련 데이터셋을 로드하기 위한 DataLoader를 생성\n","# DataLoader는 데이터셋의 마지막 배치를 버릴지 결정하는 drop_last=True 옵션을 포함하며, 배치 크기(batch_size)와 데이터셋 셔플 여부(shuffle=True)를 지정\n","# 셔플은 각 에폭마다 데이터의 순서를 무작위로 섞어, 모델이 데이터의 특정 순서에 의존하지 않도록 함\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, drop_last=True, batch_size=batch_size, shuffle=True)\n","\n","# 테스트 데이터셋을 로드하기 위한 DataLoader를 생성\n","# DataLoader는 배치 크기를 1로 설정하여 각각의 테스트 샘플을 독립적으로 평가\n","# 셔플을 비활성화(shuffle=False)하여 테스트 데이터의 순서를 유지\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","# 훈련과 테스트 DataLoader를 튜플로 묶어 다른 부분의 코드에서 쉽게 사용할 수 있도록 함\n","dataloaders = (train_dataloader, test_dataloader)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:41.692950Z","iopub.status.busy":"2022-04-08T00:15:41.692376Z","iopub.status.idle":"2022-04-08T00:15:41.700707Z","shell.execute_reply":"2022-04-08T00:15:41.699723Z","shell.execute_reply.started":"2022-04-08T00:15:41.692909Z"},"id":"LdyQz4092fRV","trusted":true},"outputs":[],"source":["# 주어진 데이터셋의 이미지를 시각화하는 역할\n","# 사용자가 설정할 수 있는 몇 가지 옵션을 포함하여 유연하게 이미지를 처리하고 표시할 수 있음\n","\n","def plotn(n, data, noisy=False, super_res=None):\n","\n","# 파라미터\n","# n: 표시할 이미지의 수\n","# data: 시각화할 데이터셋으로 각 원소는 이미지를 포함\n","# noisy: Boolean 값으로, 이미지에 잡음을 추가할지 여부를 결정\n","# super_res: 수퍼 해상도 비율을 설정하는데 이미지를 다운샘플링하기 위해 사용\n","\n","    fig, ax = plt.subplots(1, n)\n","    for i, z in enumerate(data):\n","        if i == n:\n","            break\n","\n","        # 이미지 전처리\n","        # 데이터의 각 이미지(z[0])를 적절한 형태로 재구성\n","        # 이미지가 28x28 또는 14x14 크기인지에 따라 다르게 처리\n","        # super_res가 설정된 경우, transforms.Resize를 사용하여 이미지 크기를 변경\n","\n","        preprocess = z[0].to('cpu').reshape(1, 28, 28) if z[0].shape[1] == 28 else z[0].to('cpu').reshape(1, 14, 14) if z[0].shape[1] == 14 else z[0].to('cpu')\n","        if super_res is not None:\n","            _transform = transforms.Resize((int(preprocess.shape[1] / super_res), int(preprocess.shape[2] / super_res)))\n","            preprocess = _transform(preprocess)\n","\n","        # 잡음 추가\n","        # noisy가 참인 경우, noisify 함수를 호출하여 이미지에 잡음을 추가\n","        # 일반적으로 데이터의 로버스트성을 테스트하기 위해 사용\n","\n","        if noisy:\n","            shapes = list(preprocess.shape)\n","            preprocess += noisify(shapes)\n","\n","        # 이미지 표시\n","        # matplotlib.pyplot.subplots를 사용하여 시각화할 이미지의 갯수에 맞는 subplot을 생성\n","        # 각 subplot에 이미지를 표시\n","        # ax[i].imshow(preprocess[0])는 i번째 subplot에 처리된 이미지를 표시\n","\n","        ax[i].imshow(preprocess[0])\n","        \n","    # 모든 subplot을 포함하는 그래프를 화면에 표시\n","    plt.show()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T01:10:46.485196Z","iopub.status.busy":"2022-04-08T01:10:46.484414Z","iopub.status.idle":"2022-04-08T01:10:46.489587Z","shell.execute_reply":"2022-04-08T01:10:46.488769Z","shell.execute_reply.started":"2022-04-08T01:10:46.485158Z"},"id":"FjpCEs-oWu6_","trusted":true},"outputs":[],"source":["# 이미지에 잡음을 추가하기 위해 정규 분포를 따르는 랜덤 값을 생성하는 기능을 제공\n","# 특히 이미지 데이터의 로버스트성을 테스트하거나 데이터 증강을 수행할 때 유용하게 사용\n","\n","# 입력 파라미터\n","# shapes: 잡음을 추가할 데이터의 차원을 지정\n","# 이미지 데이터의 형태(예: (1, 28, 28))와 일치해야 함\n","\n","# 잡음 생성\n","# np.random.normal: 넘파이의 정규 분포 함수를 사용하여 잡음 데이터를 생성\n","# loc=0.5: 잡음의 평균값(mean)을 0.5로 설정\n","# 이미지의 픽셀 값 범위가 0.0에서 1.0 사이인 경우, 이 평균값은 픽셀 값을 중간 수준으로 밀어놓으려는 의도\n","# scale=0.3: 표준 편차(standard deviation)를 0.3으로 설정하여, 생성되는 잡음 값의 분포 범위를 정함\n","# 이 값이 크면 잡음의 변동 폭이 커짐\n","# size=shapes: 생성될 잡음 데이터의 크기를 입력 파라미터로 받은 shapes와 동일하게 설정\n","\n","# 출력\n","# 함수는 지정된 차원과 통계적 특성을 갖는 잡음 배열을 반환\n","# 이 배열은 원본 이미지 데이터에 더해져 잡음이 섞인 이미지를 생성하는 데 사용\n","\n","def noisify(shapes):\n","    return np.random.normal(loc=0.5, scale=0.3, size=shapes)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:42.470833Z","iopub.status.busy":"2022-04-08T00:15:42.470589Z","iopub.status.idle":"2022-04-08T00:15:42.855919Z","shell.execute_reply":"2022-04-08T00:15:42.853605Z","shell.execute_reply.started":"2022-04-08T00:15:42.470808Z"},"id":"NeWJoiFC4A6J","outputId":"e680eb07-bf94-4301-8742-852103885624","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /Users/woojinpaik/.pyenv/versions/3.11.5/lib/python3.11/site-packages (1.26.4)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 5 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["%pip install --upgrade numpy\n","\n","plotn(5, train_dataset)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:42.857823Z","iopub.status.busy":"2022-04-08T00:15:42.857536Z","iopub.status.idle":"2022-04-08T00:15:42.866008Z","shell.execute_reply":"2022-04-08T00:15:42.865357Z","shell.execute_reply.started":"2022-04-08T00:15:42.857787Z"},"id":"NnI2YvOg4DbT","trusted":true},"outputs":[],"source":["#  PyTorch의 nn.Module을 상속받아 컨볼루션 신경망을 구현\n","# 주로 이미지 데이터의 차원을 축소하거나 중요한 특징을 추출하는 데 사용\n","\n","class Encoder(nn.Module):\n","    def __init__(self): # 클래스 초기화\n","        super().__init__() # Python의 상속 메커니즘을 통해 nn.Module의 초기화 메서드를 호출\n","        \n","        # 첫 번째 컨볼루션 레이어로, 입력 채널 1개를 받아 16개의 출력 채널로 매핑\n","        # 커널 크기는 3x3이며, padding='same'은 입력과 출력의 공간적 차원을 동일하게 유지\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding='same')\n","        \n","        # 첫 번째 맥스 풀링 레이어로, 2x2의 윈도우 크기를 사용해 공간적 차원을 줄임\n","        # 특징의 공간적 크기를 감소시키면서 중요한 정보는 유지\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2))\n","        self.conv2 = nn.Conv2d(16, 8, kernel_size=(3, 3), padding='same')\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2))\n","        self.conv3 = nn.Conv2d(8, 8, kernel_size=(3, 3), padding='same')\n","        \n","        # 세 번째 맥스 풀링 레이어로, 패딩이 (1, 1)로 설정되어 있어 풀링 과정에서 입력의 가장자리 부분도 고려\n","        self.maxpool3 = nn.MaxPool2d(kernel_size=(2, 2), padding=(1, 1))\n","        \n","        # 활성화 함수로 ReLU(Rectified Linear Unit)를 사용\n","        # ReLU는 음수 입력에 대해 0을 출력하고, 양수 입력에 대해서는 입력을 그대로 출력\n","        self.relu = nn.ReLU()\n","\n","    # 입력 이미지 input은 차례대로 각 컨볼루션 레이어와 ReLU 활성화 함수를 거친 다음, 맥스 풀링 레이어를 통과\n","    # 이 과정을 통해 입력 데이터의 차원이 축소되고, 중요한 특징들을 추출\n","    def forward(self, input):\n","        \n","        # hidden1, hidden2, encoded는 각각의 중간 단계에서 생성된 특징 맵(feature map)을 나타냄\n","        hidden1 = self.maxpool1(self.relu(self.conv1(input)))\n","        hidden2 = self.maxpool2(self.relu(self.conv2(hidden1)))\n","        encoded = self.maxpool3(self.relu(self.conv3(hidden2)))\n","        \n","        # 최종적으로 encoded라는 이름의 특징 맵이 반환됨\n","        # 이 특징 맵은 입력 이미지가 고차원에서 저차원으로 정보가 압축된 결과\n","        return encoded"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:43.169659Z","iopub.status.busy":"2022-04-08T00:15:43.169239Z","iopub.status.idle":"2022-04-08T00:15:43.178700Z","shell.execute_reply":"2022-04-08T00:15:43.178010Z","shell.execute_reply.started":"2022-04-08T00:15:43.169622Z"},"id":"mZGB4Vr47478","trusted":true},"outputs":[],"source":["# PyTorch의 nn.Module을 상속받아 이미지나 다른 고차원 데이터를 복원하는 딥러닝 모델의 디코더 부분을 구현\n","# 압축된 특징을 입력으로 받아 원본 데이터의 고차원 형태로 복원하는 역할\n","\n","class Decoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        # 입력 채널 8개를 받아 출력 채널 8개로 매핑하는 첫 번째 컨볼루션 레이어\n","        # 커널 크기는 3x3이며, 패딩은 'same'으로 설정\n","        self.conv1 = nn.Conv2d(8, 8, kernel_size=(3, 3), padding='same')\n","        \n","        # 첫 번째 업샘플링 레이어로, 공간적 차원을 2배로 확대\n","        # 맥스 풀링으로 감소된 차원을 다시 확대하는 데 사용\n","        self.upsample1 = nn.Upsample(scale_factor=(2, 2))\n","        self.conv2 = nn.Conv2d(8, 8, kernel_size=(3, 3), padding='same')\n","        self.upsample2 = nn.Upsample(scale_factor=(2, 2))\n","        self.conv3 = nn.Conv2d(8, 16, kernel_size=(3, 3))\n","        self.upsample3 = nn.Upsample(scale_factor=(2, 2))\n","        self.conv4 = nn.Conv2d(16, 1, kernel_size=(3, 3), padding='same')\n","        self.relu = nn.ReLU()\n","        \n","        # 출력 레이어에 시그모이드 활성화 함수를 사용\n","        # 최종 출력을 [0, 1] 범위로 제한하기 위해 사용\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, input):\n","        hidden1 = self.upsample1(self.relu(self.conv1(input)))\n","        hidden2 = self.upsample2(self.relu(self.conv2(hidden1)))\n","        hidden3 = self.upsample3(self.relu(self.conv3(hidden2)))\n","        decoded = self.sigmoid(self.conv4(hidden3))\n","        \n","        #  마지막 컨볼루션 레이어와 시그모이드 활성화 함수를 거쳐 최종적으로 복원된 이미지\n","        # 원본 데이터와 비슷한 형태를 가지며, 값의 범위는 [0, 1]\n","        return decoded"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:43.593770Z","iopub.status.busy":"2022-04-08T00:15:43.593178Z","iopub.status.idle":"2022-04-08T00:15:43.600152Z","shell.execute_reply":"2022-04-08T00:15:43.599205Z","shell.execute_reply.started":"2022-04-08T00:15:43.593731Z"},"id":"SDGiAPhbBBLY","trusted":true},"outputs":[],"source":["# 이미지 또는 다른 데이터 유형의 자동 인코딩과 디코딩을 수행하는 PyTorch의 nn.Module을 상속받은 모듈\n","# 입력 데이터를 압축(인코딩)하고 이를 다시 복원(디코딩)하는 기능을 갖추고 있으며, 선택적으로 고해상도(슈퍼 해상도) 인코딩을 지원\n","class AutoEncoder(nn.Module):\n","    def __init__(self, super_resolution=False):\n","        super().__init__()\n","        \n","        # Boole 매개변수를 통해 모델이 일반 인코더를 사용할지, 아니면 고해상도를 위한 특별한 인코더(SuperResolutionEncoder)를 사용할지 결정\n","        if not super_resolution:\n","            self.encoder = Encoder()\n","        else:\n","            self.encoder = SuperResolutionEncoder()\n","        \n","        # 데이터를 복원하는 Decoder 클래스의 인스턴스를 생성\n","        self.decoder = Decoder()\n","\n","    def forward(self, input):\n","        \n","        # self.encoder(input)을 호출하여 입력 데이터를 인코딩(압축)\n","        # 입력 데이터의 차원을 줄이면서 중요한 특징을 유지\n","        encoded = self.encoder(input)\n","        \n","        # 인코딩된 데이터를 다시 복원\n","        # 복원 과정은 인코딩 과정에서 손실된 정보를 최소화하면서 원본 데이터와 비슷한 형태로 재구성\n","        decoded = self.decoder(encoded)\n","        return decoded"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:44.069329Z","iopub.status.busy":"2022-04-08T00:15:44.068875Z","iopub.status.idle":"2022-04-08T00:15:44.079486Z","shell.execute_reply":"2022-04-08T00:15:44.078748Z","shell.execute_reply.started":"2022-04-08T00:15:44.069289Z"},"id":"nZyG_mNu_Pnc","trusted":true},"outputs":[],"source":["# PyTorch를 사용해 오토인코더 모델을 생성하고 설정하는 과정\n","\n","# AutoEncoder 클래스의 인스턴스를 생성\n","# 입력 데이터를 압축하는 인코더와 압축된 데이터를 복원하는 디코더를 포함\n","model = AutoEncoder().to(device)\n","\n","# 옵티마이저 설정\n","# Adam 최적화 알고리즘을 사용\n","# Adam은 경사 하강법의 변형으로, 각 매개변수의 학습 속도를 적응적으로 조정\n","# model.parameters(): 최적화할 모델 매개변수를 가져옴\n","# lr: 학습률(learning rate)로, 모델이 학습하는 속도를 결정\n","# eps: Adam 최적화에서 안정성을 보장하기 위한 매우 작은 값\n","optimizer = optim.Adam(model.parameters(), lr=lr, eps=eps)\n","\n","# 손실함수\n","# nn.BCELoss(): 이진 교차 엔트로피 손실(Binary Cross-Entropy Loss)을 사용\n","# 주로 이진 분류 문제에서 사용되며, 오토인코더의 출력과 실제 값 사이의 차이를 측정하는 데 적합\n","# 출력이 [0, 1] 범위인 경우(시그모이드 활성화 함수 사용)\n","# BCELoss는 효과적\n","loss_fn = nn.BCELoss()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T01:25:58.828126Z","iopub.status.busy":"2022-04-08T01:25:58.827634Z","iopub.status.idle":"2022-04-08T01:25:58.844248Z","shell.execute_reply":"2022-04-08T01:25:58.843270Z","shell.execute_reply.started":"2022-04-08T01:25:58.828092Z"},"id":"iiIy87v2_rUr","trusted":true},"outputs":[],"source":["# 오토인코더 모델의 훈련 과정을 제어하는 함수\n","# 입력 데이터에 잡음을 추가하거나 수퍼 해상도 변환을 적용한 뒤, 모델을 통해 데이터를 인코딩 및 디코딩하여 원본 데이터와의 손실을 최소화하는 방식으로 작동\n","\n","# 훈련 및 테스트 과정\n","# 각 에폭마다 모델을 훈련 모드로 설정한 후, 훈련 데이터로더를 통해 배치를 순차적으로 처리\n","# 입력 이미지에 수퍼 해상도 또는 잡음을 추가한 뒤, 모델을 통해 결과를 예측하고 손실을 계산\n","# 손실에 대한 역전파를 수행하고 최적화기로 가중치를 업데이트\n","# 테스트 과정은 훈련과 유사하나, 모델을 평가 모드로 설정하고 역전파나 가중치 업데이트를 수행하지 않음\n","# 테스트 데이터로더를 통해 모델의 성능을 평가\n","\n","# 손실 및 진행 상태 추적\n","# train_loss와 test_loss는 각각 훈련과 테스트 데이터셋에 대한 손실을 누적\n","# 각 에폭의 끝에서 평균 손실을 계산하고, 이를 tqdm 반복자의 상태 바에 출력하여 훈련 과정을 시각적으로 모니터\n","\n","def train(dataloaders, model, loss_fn, optimizer, epochs, device, noisy=None, super_res=None):\n","    tqdm_iter = tqdm(range(epochs))\n","    train_dataloader, test_dataloader = dataloaders[0], dataloaders[1]\n","\n","    for epoch in tqdm_iter:\n","        model.train()\n","        train_loss = 0.0\n","        test_loss = 0.0\n","\n","        for batch in train_dataloader:\n","            imgs, labels = batch\n","            shapes = list(imgs.shape)\n","\n","            if super_res is not None:\n","                shapes[2], shapes[3] = int(shapes[2] / super_res), int(shapes[3] / super_res)\n","                _transform = transforms.Resize((shapes[2], shapes[3]))\n","                imgs_transformed = _transform(imgs)\n","                imgs_transformed = imgs_transformed.to(device)\n","\n","            imgs = imgs.to(device)\n","            labels = labels.to(device)\n","\n","            if noisy is not None:\n","                noisy_tensor = noisy[0]\n","            else:\n","                noisy_tensor = torch.zeros(tuple(shapes)).to(device)\n","\n","            if super_res is None:\n","                imgs_noisy = imgs + noisy_tensor\n","            else:\n","                imgs_noisy = imgs_transformed + noisy_tensor\n","\n","            imgs_noisy = torch.clamp(imgs_noisy, 0., 1.)\n","\n","            preds = model(imgs_noisy)\n","            loss = loss_fn(preds, imgs)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            for batch in test_dataloader:\n","                imgs, labels = batch\n","                shapes = list(imgs.shape)\n","\n","                if super_res is not None:\n","                    shapes[2], shapes[3] = int(shapes[2] / super_res), int(shapes[3] / super_res)\n","                    _transform = transforms.Resize((shapes[2], shapes[3]))\n","                    imgs_transformed = _transform(imgs)\n","                    imgs_transformed = imgs_transformed.to(device)\n","\n","\n","                imgs = imgs.to(device)\n","                labels = labels.to(device)\n","\n","                if noisy is not None:\n","                    test_noisy_tensor = noisy[1]\n","                else:\n","                    test_noisy_tensor = torch.zeros(tuple(shapes)).to(device)\n","\n","                if super_res is None:\n","                    imgs_noisy = imgs + test_noisy_tensor\n","                else:\n","                    imgs_noisy = imgs_transformed + test_noisy_tensor\n","\n","                imgs_noisy = torch.clamp(imgs_noisy, 0., 1.)\n","\n","                preds = model(imgs_noisy)\n","                loss = loss_fn(preds, imgs)\n","\n","                test_loss += loss.item()\n","\n","        train_loss /= len(train_dataloader)\n","        test_loss /= len(test_dataloader)\n","\n","        tqdm_dct = {'train loss:': train_loss, 'test loss:': test_loss}\n","        tqdm_iter.set_postfix(tqdm_dct, refresh=True)\n","        tqdm_iter.refresh()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:15:44.969616Z","iopub.status.busy":"2022-04-08T00:15:44.969345Z","iopub.status.idle":"2022-04-08T00:22:34.533469Z","shell.execute_reply":"2022-04-08T00:22:34.532667Z","shell.execute_reply.started":"2022-04-08T00:15:44.969587Z"},"id":"PMqO8eOxCemz","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/30 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 30/30 [19:47<00:00, 39.58s/it, train loss:=0.104, test loss:=0.104]\n"]}],"source":["train(dataloaders, model, loss_fn, optimizer, epochs, device)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:22:34.535491Z","iopub.status.busy":"2022-04-08T00:22:34.535154Z","iopub.status.idle":"2022-04-08T00:22:35.701465Z","shell.execute_reply":"2022-04-08T00:22:35.700797Z","shell.execute_reply.started":"2022-04-08T00:22:34.535452Z"},"id":"kR3n0EnjOts0","trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 5 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 5 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# 모델을 평가 모드로 전환\n","# 드롭아웃이나 배치 정규화와 같은 훈련 중에만 활성화되어야 하는 기능들을 비활성화\n","# 모델이 일관된 예측을 생성할 수 있음\n","\n","model.eval()\n","predictions = [] # 예측 결과를 저장할 빈 리스트를 초기화\n","plots = 5\n","\n","# 테스트 데이터셋을 순회하면서 각 샘플을 처리\n","# i는 인덱스, data는 각 데이터 포인트\n","for i, data in enumerate(test_dataset):\n","    if i == plots:\n","        break\n","    \n","    # 모델에 입력 데이터를 전달하고, 예측 결과를 predictions 리스트에 추가\n","    # data[0].to(device).unsqueeze(0)은 이미지 데이터를 모델 입력에 맞게 처리\n","    # 데이터를 대상 장치로 이동시키고, 배치 차원을 추가\n","    # detach().cpu()는 계산 그래프에서 예측 결과를 분리하고 CPU로 이동\n","    predictions.append(model(data[0].to(device).unsqueeze(0)).detach().cpu())\n","plotn(plots, test_dataset) # 테스트 데이터셋에서 처음 5개 샘플을 시각화\n","plotn(plots, predictions) # 모델이 생성한 예측 결과를 시각화"]},{"cell_type":"markdown","metadata":{"id":"tjdFt03rULX-"},"source":["## Denoising\n","\n","Autoencoders can be effectively used to remove noise from images. In order to train denoiser, we will start with noise-free images, and add artificial noise to them. Then, we will feed autoencoder with noisy images as input, and noise-free images as output.\n","\n","Let's see how this works for MNIST:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T01:26:06.259333Z","iopub.status.busy":"2022-04-08T01:26:06.259061Z","iopub.status.idle":"2022-04-08T01:26:06.799264Z","shell.execute_reply":"2022-04-08T01:26:06.798609Z","shell.execute_reply.started":"2022-04-08T01:26:06.259303Z"},"id":"1Yj9ZRDmUPxX","outputId":"7e3a1624-4326-41a2-8f94-3d41e11cd411","trusted":true},"outputs":[],"source":["plotn(5, train_dataset, noisy=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T01:55:45.901599Z","iopub.status.busy":"2022-04-08T01:55:45.901279Z","iopub.status.idle":"2022-04-08T01:55:45.910993Z","shell.execute_reply":"2022-04-08T01:55:45.909858Z","shell.execute_reply.started":"2022-04-08T01:55:45.901533Z"},"id":"qxo8NDLLUvut","trusted":true},"outputs":[],"source":["model = AutoEncoder().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr, eps=eps)\n","loss_fn = nn.BCELoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T01:55:47.302746Z","iopub.status.busy":"2022-04-08T01:55:47.302202Z","iopub.status.idle":"2022-04-08T01:55:47.316135Z","shell.execute_reply":"2022-04-08T01:55:47.315439Z","shell.execute_reply.started":"2022-04-08T01:55:47.302710Z"},"trusted":true},"outputs":[],"source":["noisy_tensor = torch.FloatTensor(noisify([256, 1, 28, 28])).to(device)\n","test_noisy_tensor = torch.FloatTensor(noisify([1, 1, 28, 28])).to(device)\n","noisy_tensors = (noisy_tensor, test_noisy_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T01:55:47.956516Z","iopub.status.busy":"2022-04-08T01:55:47.956250Z","iopub.status.idle":"2022-04-08T02:18:17.428958Z","shell.execute_reply":"2022-04-08T02:18:17.428239Z","shell.execute_reply.started":"2022-04-08T01:55:47.956489Z"},"id":"JAYzgfoTUBHM","trusted":true},"outputs":[],"source":["train(dataloaders, model, loss_fn, optimizer, 100, device, noisy=noisy_tensors)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T02:18:17.430982Z","iopub.status.busy":"2022-04-08T02:18:17.430570Z","iopub.status.idle":"2022-04-08T02:18:18.235054Z","shell.execute_reply":"2022-04-08T02:18:18.234364Z","shell.execute_reply.started":"2022-04-08T02:18:17.430943Z"},"id":"IaPfyJ0SV7XY","trusted":true},"outputs":[],"source":["model.eval()\n","predictions = []\n","noise = []\n","plots = 5\n","for i, data in enumerate(test_dataset):\n","    if i == plots:\n","        break\n","    shapes = data[0].shape\n","    noisy_data = data[0] + test_noisy_tensor[0].detach().cpu()\n","    noise.append(noisy_data)\n","    predictions.append(model(noisy_data.to(device).unsqueeze(0)).detach().cpu())\n","plotn(plots, noise)\n","plotn(plots, predictions)"]},{"cell_type":"markdown","metadata":{"id":"yxFKm_OxdqfO"},"source":["## Super-Resolution\n","\n","Similarly to denoiser, we can train autoencoders to increase the resolution of the image. To train super-resolution network, we will start with high-resolution images, and automatically downscale them to produce network inputs. We will then feed autoencoder with small images as inputs and high-resolution images as outputs.\n","\n","For that let's downscale image to 14x14 at train."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:31:45.310634Z","iopub.status.busy":"2022-04-08T00:31:45.310384Z","iopub.status.idle":"2022-04-08T00:31:45.688313Z","shell.execute_reply":"2022-04-08T00:31:45.687614Z","shell.execute_reply.started":"2022-04-08T00:31:45.310597Z"},"id":"trida5guu9js","outputId":"3dd13ba4-0351-4982-c162-578735af23f8","trusted":true},"outputs":[],"source":["super_res_koeff = 2.0\n","plotn(5, train_dataset, super_res=super_res_koeff)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:31:45.690022Z","iopub.status.busy":"2022-04-08T00:31:45.689620Z","iopub.status.idle":"2022-04-08T00:31:45.698351Z","shell.execute_reply":"2022-04-08T00:31:45.697586Z","shell.execute_reply.started":"2022-04-08T00:31:45.689984Z"},"id":"9vC57e-rei4p","trusted":true},"outputs":[],"source":["class SuperResolutionEncoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding='same')\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2))\n","        self.conv2 = nn.Conv2d(16, 8, kernel_size=(3, 3), padding='same')\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), padding=(1, 1))\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, input):\n","        hidden1 = self.maxpool1(self.relu(self.conv1(input)))\n","        encoded = self.maxpool2(self.relu(self.conv2(hidden1)))\n","        return encoded"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:31:45.699896Z","iopub.status.busy":"2022-04-08T00:31:45.699576Z","iopub.status.idle":"2022-04-08T00:31:45.714684Z","shell.execute_reply":"2022-04-08T00:31:45.713859Z","shell.execute_reply.started":"2022-04-08T00:31:45.699857Z"},"id":"d78J288qe5qJ","trusted":true},"outputs":[],"source":["model = AutoEncoder(super_resolution=True).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr, eps=eps)\n","loss_fn = nn.BCELoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:31:45.718432Z","iopub.status.busy":"2022-04-08T00:31:45.718047Z","iopub.status.idle":"2022-04-08T00:38:29.683824Z","shell.execute_reply":"2022-04-08T00:38:29.683115Z","shell.execute_reply.started":"2022-04-08T00:31:45.718402Z"},"id":"CJ_zcN5Je6I-","outputId":"2311bd2a-55c6-4fdf-c4d2-37f0426fb61c","trusted":true},"outputs":[],"source":["train(dataloaders, model, loss_fn, optimizer, epochs, device, super_res=2.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:29.687844Z","iopub.status.busy":"2022-04-08T00:38:29.687353Z","iopub.status.idle":"2022-04-08T00:38:30.585026Z","shell.execute_reply":"2022-04-08T00:38:30.584352Z","shell.execute_reply.started":"2022-04-08T00:38:29.687803Z"},"id":"YsVfcCKKfjv1","outputId":"e7e029b0-b403-4feb-f869-d6367957741a","trusted":true},"outputs":[],"source":["model.eval()\n","predictions = []\n","plots = 5\n","shapes = test_dataset[0][0].shape\n","\n","for i, data in enumerate(test_dataset):\n","    if i == plots:\n","        break\n","    _transform = transforms.Resize((int(shapes[1] / super_res_koeff), int(shapes[2] / super_res_koeff)))\n","    predictions.append(model(_transform(data[0]).to(device).unsqueeze(0)).detach().cpu())\n","plotn(plots, test_dataset, super_res=super_res_koeff)\n","plotn(plots, predictions)"]},{"cell_type":"markdown","metadata":{"id":"A3DOJU1-gTJV"},"source":["# [Variational Auto-Encoders (VAE)](https://arxiv.org/abs/1906.02691)\n","\n","Traditional autoencoders reduce the dimension of the input data somehow, figuring out the important features of input images. However, latent vectors often do not make much sense. In other words, taking MNIST dataset as an example, figuring out which digits correspond to different latent vectors is not an easy task, because close latent vectors would not necessarily correspond to the same digits. \n","\n","On the other hand, to train *generative* models it is better to have some understanding of the latent space. This idea leads us to **variational auto-encoder** (VAE).\n","\n","VAE is the autoencoder that learns to predict *statistical distribution* of the latent parameters, so-called **latent distribution**. For example, we can assume that latent vectors would be distributed as $N(\\mathrm{z\\_mean},e^{\\mathrm{z\\_log}})$, where $\\mathrm{z\\_mean}, \\mathrm{z\\_log} \\in\\mathbb{R}^d$. Encoder in VAE learns to predict those parameters, and then decoder takes a random vector from this distribution to reconstruct the object.\n","\n","To summarize:\n","\n"," * From input vector, we predict `z_mean` and `z_log` (instead of predicting the standard deviation itself, we predict it's logarithm)\n"," * We sample a vector `sample(z_val in code)` from the distribution $N(\\mathrm{z\\_mean},e^{\\mathrm{z\\_log\\_sigma}})$\n"," * Decoder tries to decode the original image using `sample` as an input vector"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.586882Z","iopub.status.busy":"2022-04-08T00:38:30.586394Z","iopub.status.idle":"2022-04-08T00:38:30.595011Z","shell.execute_reply":"2022-04-08T00:38:30.594391Z","shell.execute_reply.started":"2022-04-08T00:38:30.586844Z"},"id":"aT_GEWeU409I","trusted":true},"outputs":[],"source":["class VAEEncoder(nn.Module):\n","    def __init__(self, device):\n","        super().__init__()\n","        self.intermediate_dim = 512\n","        self.latent_dim = 2\n","        self.linear = nn.Linear(784, self.intermediate_dim)\n","        self.z_mean = nn.Linear(self.intermediate_dim, self.latent_dim)\n","        self.z_log = nn.Linear(self.intermediate_dim, self.latent_dim)\n","        self.relu = nn.ReLU()\n","        self.device = device\n","\n","    def forward(self, input):\n","        bs = input.shape[0]\n","\n","        hidden = self.relu(self.linear(input))\n","        z_mean = self.z_mean(hidden)\n","        z_log = self.z_log(hidden)\n","\n","        eps = torch.FloatTensor(np.random.normal(size=(bs, self.latent_dim))).to(device)\n","        z_val = z_mean + torch.exp(z_log) * eps\n","        return z_mean, z_log, z_val"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.596177Z","iopub.status.busy":"2022-04-08T00:38:30.595932Z","iopub.status.idle":"2022-04-08T00:38:30.606805Z","shell.execute_reply":"2022-04-08T00:38:30.606054Z","shell.execute_reply.started":"2022-04-08T00:38:30.596144Z"},"id":"XWi4oCcq409p","trusted":true},"outputs":[],"source":["class VAEDecoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.intermediate_dim = 512\n","        self.latent_dim = 2\n","        self.linear = nn.Linear(self.latent_dim, self.intermediate_dim)\n","        self.output = nn.Linear(self.intermediate_dim, 784)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, input):\n","        hidden = self.relu(self.linear(input))\n","        decoded = self.sigmoid(self.output(hidden))\n","        return decoded"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.608150Z","iopub.status.busy":"2022-04-08T00:38:30.607836Z","iopub.status.idle":"2022-04-08T00:38:30.618464Z","shell.execute_reply":"2022-04-08T00:38:30.617740Z","shell.execute_reply.started":"2022-04-08T00:38:30.608114Z"},"id":"WukDYQ9f409p","trusted":true},"outputs":[],"source":["class VAEAutoEncoder(nn.Module):\n","    def __init__(self, device):\n","        super().__init__()\n","        self.encoder = VAEEncoder(device)\n","        self.decoder = VAEDecoder()\n","        self.z_vals = None\n","\n","    def forward(self, input):\n","        bs, c, h, w = input.shape[0], input.shape[1], input.shape[2], input.shape[3]\n","        input = input.view(bs, -1)\n","        encoded = self.encoder(input)\n","        self.z_vals = encoded\n","        decoded = self.decoder(encoded[2])\n","        return decoded\n","    \n","    def get_zvals(self):\n","        return self.z_vals"]},{"cell_type":"markdown","metadata":{},"source":["Variational auto-encoders use complex loss function that consists of two parts:\n","* **Reconstruction loss** is the loss function that shows how close reconstructed image is to the target (can be MSE). It is the same loss function as in normal autoencoders.\n","* **KL loss**, which ensures that latent variable distributions stays close to normal distribution. It is based on the notion of [Kullback-Leibler divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) - a metric to estimate how similar two statistical distributions are."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.620121Z","iopub.status.busy":"2022-04-08T00:38:30.619868Z","iopub.status.idle":"2022-04-08T00:38:30.627984Z","shell.execute_reply":"2022-04-08T00:38:30.627265Z","shell.execute_reply.started":"2022-04-08T00:38:30.620089Z"},"trusted":true},"outputs":[],"source":["def vae_loss(preds, targets, z_vals):\n","    mse = nn.MSELoss()\n","    reconstruction_loss = mse(preds, targets.view(targets.shape[0], -1)) * 784.0\n","    temp = 1.0 + z_vals[1] - torch.square(z_vals[0]) - torch.exp(z_vals[1])\n","    kl_loss = -0.5 * torch.sum(temp, axis=-1)\n","    return torch.mean(reconstruction_loss + kl_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.629709Z","iopub.status.busy":"2022-04-08T00:38:30.629192Z","iopub.status.idle":"2022-04-08T00:38:30.648407Z","shell.execute_reply":"2022-04-08T00:38:30.647801Z","shell.execute_reply.started":"2022-04-08T00:38:30.629671Z"},"trusted":true},"outputs":[],"source":["model = VAEAutoEncoder(device).to(device)\n","optimizer = optim.RMSprop(model.parameters(), lr=lr, eps=eps)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.649871Z","iopub.status.busy":"2022-04-08T00:38:30.649560Z","iopub.status.idle":"2022-04-08T00:38:30.659437Z","shell.execute_reply":"2022-04-08T00:38:30.658769Z","shell.execute_reply.started":"2022-04-08T00:38:30.649835Z"},"trusted":true},"outputs":[],"source":["def train_vae(dataloaders, model, optimizer, epochs, device):\n","    tqdm_iter = tqdm(range(epochs))\n","    train_dataloader, test_dataloader = dataloaders[0], dataloaders[1]\n","\n","    for epoch in tqdm_iter:\n","        model.train()\n","        train_loss = 0.0\n","        test_loss = 0.0\n","\n","        for batch in train_dataloader:\n","            imgs, labels = batch\n","            imgs = imgs.to(device)\n","            labels = labels.to(device)\n","\n","            preds = model(imgs)\n","            z_vals = model.get_zvals()\n","            loss = vae_loss(preds, imgs, z_vals)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            for batch in test_dataloader:\n","                imgs, labels = batch\n","                imgs = imgs.to(device)\n","                labels = labels.to(device)\n","\n","                preds = model(imgs)\n","                z_vals = model.get_zvals()\n","                loss = vae_loss(preds, imgs, z_vals)\n","\n","                test_loss += loss.item()\n","\n","        train_loss /= len(train_dataloader)\n","        test_loss /= len(test_dataloader)\n","\n","        tqdm_dct = {'train loss:': train_loss, 'test loss:': test_loss}\n","        tqdm_iter.set_postfix(tqdm_dct, refresh=True)\n","        tqdm_iter.refresh()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:38:30.661256Z","iopub.status.busy":"2022-04-08T00:38:30.660812Z","iopub.status.idle":"2022-04-08T00:43:25.674280Z","shell.execute_reply":"2022-04-08T00:43:25.673608Z","shell.execute_reply.started":"2022-04-08T00:38:30.661216Z"},"trusted":true},"outputs":[],"source":["train_vae(dataloaders, model, optimizer, epochs, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:25.676045Z","iopub.status.busy":"2022-04-08T00:43:25.675634Z","iopub.status.idle":"2022-04-08T00:43:26.429255Z","shell.execute_reply":"2022-04-08T00:43:26.428437Z","shell.execute_reply.started":"2022-04-08T00:43:25.676007Z"},"trusted":true},"outputs":[],"source":["model.eval()\n","predictions = []\n","plots = 5\n","for i, data in enumerate(test_dataset):\n","    if i == plots:\n","        break\n","    predictions.append(model(data[0].to(device).unsqueeze(0)).view(1, 28, 28).detach().cpu())\n","plotn(plots, test_dataset)\n","plotn(plots, predictions)"]},{"cell_type":"markdown","metadata":{},"source":["# [Adversarial Auto-Encoders (AAE)](https://arxiv.org/abs/1511.05644)"]},{"cell_type":"markdown","metadata":{},"source":["Adversarial Auto-Encoders is a **combination** of Generative Adversarial Networks and Variational Auto-Encoders. \n","\n","Encoder will be the generator, discriminator will learn to distinguish the real images encoder output from generated ones. Encoder output is a distribution, from this output decoder will try decode image.\n","\n","In this approach we have **three loss functions**: generator loss, discriminator loss from GAN's and reconstruction loss from VAE."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.430881Z","iopub.status.busy":"2022-04-08T00:43:26.430435Z","iopub.status.idle":"2022-04-08T00:43:26.437703Z","shell.execute_reply":"2022-04-08T00:43:26.437030Z","shell.execute_reply.started":"2022-04-08T00:43:26.430837Z"},"trusted":true},"outputs":[],"source":["class AAEEncoder(nn.Module):\n","    def __init__(self, input_dim, inter_dim, latent_dim):\n","        super().__init__()\n","        self.linear1 = nn.Linear(input_dim, inter_dim)\n","        self.linear2 = nn.Linear(inter_dim, inter_dim)\n","        self.linear3 = nn.Linear(inter_dim, inter_dim)\n","        self.linear4 = nn.Linear(inter_dim, latent_dim)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, input):\n","        hidden1 = self.relu(self.linear1(input))\n","        hidden2 = self.relu(self.linear2(hidden1))\n","        hidden3 = self.relu(self.linear3(hidden2))\n","        encoded = self.linear4(hidden3)\n","        return encoded"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.439383Z","iopub.status.busy":"2022-04-08T00:43:26.438973Z","iopub.status.idle":"2022-04-08T00:43:26.450914Z","shell.execute_reply":"2022-04-08T00:43:26.450120Z","shell.execute_reply.started":"2022-04-08T00:43:26.439339Z"},"trusted":true},"outputs":[],"source":["class AAEDecoder(nn.Module):\n","    def __init__(self, latent_dim, inter_dim, output_dim):\n","        super().__init__()\n","        self.linear1 = nn.Linear(latent_dim, inter_dim)\n","        self.linear2 = nn.Linear(inter_dim, inter_dim)\n","        self.linear3 = nn.Linear(inter_dim, inter_dim)\n","        self.linear4 = nn.Linear(inter_dim, output_dim)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self, input):\n","        hidden1 = self.relu(self.linear1(input))\n","        hidden2 = self.relu(self.linear2(hidden1))\n","        hidden3 = self.relu(self.linear3(hidden2))\n","        decoded = self.sigmoid(self.linear4(hidden3))\n","        return decoded"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.453838Z","iopub.status.busy":"2022-04-08T00:43:26.453577Z","iopub.status.idle":"2022-04-08T00:43:26.462584Z","shell.execute_reply":"2022-04-08T00:43:26.461884Z","shell.execute_reply.started":"2022-04-08T00:43:26.453814Z"},"trusted":true},"outputs":[],"source":["class AAEDiscriminator(nn.Module):\n","    def __init__(self, latent_dim, inter_dim):\n","        super().__init__()\n","        self.latent_dim = latent_dim\n","        self.inter_dim = inter_dim\n","        self.linear1 = nn.Linear(latent_dim, inter_dim)\n","        self.linear2 = nn.Linear(inter_dim, inter_dim)\n","        self.linear3 = nn.Linear(inter_dim, inter_dim)\n","        self.linear4 = nn.Linear(inter_dim, inter_dim)\n","        self.linear5 = nn.Linear(inter_dim, 1)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self, input):\n","        hidden1 = self.relu(self.linear1(input))\n","        hidden2 = self.relu(self.linear2(hidden1))\n","        hidden3 = self.relu(self.linear3(hidden2))\n","        hidden4 = self.relu(self.linear4(hidden3))\n","        decoded = self.sigmoid(self.linear4(hidden4))\n","        return decoded\n","    \n","    def get_dims(self):\n","        return self.latent_dim, self.inter_dim\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.464666Z","iopub.status.busy":"2022-04-08T00:43:26.464215Z","iopub.status.idle":"2022-04-08T00:43:26.474821Z","shell.execute_reply":"2022-04-08T00:43:26.474117Z","shell.execute_reply.started":"2022-04-08T00:43:26.464628Z"},"trusted":true},"outputs":[],"source":["input_dims = 784\n","inter_dims = 1000\n","latent_dims = 150"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.477879Z","iopub.status.busy":"2022-04-08T00:43:26.477199Z","iopub.status.idle":"2022-04-08T00:43:26.537720Z","shell.execute_reply":"2022-04-08T00:43:26.537052Z","shell.execute_reply.started":"2022-04-08T00:43:26.477797Z"},"trusted":true},"outputs":[],"source":["aae_encoder = AAEEncoder(input_dims, inter_dims, latent_dims).to(device)\n","aae_decoder = AAEDecoder(latent_dims, inter_dims, input_dims).to(device)\n","aae_discriminator = AAEDiscriminator(latent_dims, int(inter_dims / 2)).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.539592Z","iopub.status.busy":"2022-04-08T00:43:26.539109Z","iopub.status.idle":"2022-04-08T00:43:26.543949Z","shell.execute_reply":"2022-04-08T00:43:26.543156Z","shell.execute_reply.started":"2022-04-08T00:43:26.539555Z"},"trusted":true},"outputs":[],"source":["lr = 1e-4\n","regularization_lr = 5e-5"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.545858Z","iopub.status.busy":"2022-04-08T00:43:26.545404Z","iopub.status.idle":"2022-04-08T00:43:26.553377Z","shell.execute_reply":"2022-04-08T00:43:26.552700Z","shell.execute_reply.started":"2022-04-08T00:43:26.545825Z"},"trusted":true},"outputs":[],"source":["optim_encoder = optim.Adam(aae_encoder.parameters(), lr=lr)\n","optim_encoder_regularization = optim.Adam(aae_encoder.parameters(), lr=regularization_lr)\n","optim_decoder = optim.Adam(aae_decoder.parameters(), lr=lr)\n","optim_discriminator = optim.Adam(aae_discriminator.parameters(), lr=regularization_lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.555945Z","iopub.status.busy":"2022-04-08T00:43:26.555659Z","iopub.status.idle":"2022-04-08T00:43:26.576630Z","shell.execute_reply":"2022-04-08T00:43:26.575929Z","shell.execute_reply.started":"2022-04-08T00:43:26.555908Z"},"trusted":true},"outputs":[],"source":["def train_aae(dataloaders, models, optimizers, epochs, device):\n","    tqdm_iter = tqdm(range(epochs))\n","    train_dataloader, test_dataloader = dataloaders[0], dataloaders[1]\n","    \n","    enc, dec, disc = models[0], models[1], models[2]\n","    optim_enc, optim_enc_reg, optim_dec, optim_disc = optimizers[0], optimizers[1], optimizers[2], optimizers[3]\n","    \n","    eps = 1e-9\n","\n","    for epoch in tqdm_iter:\n","        enc.train()\n","        dec.train()\n","        disc.train()\n","\n","        train_reconst_loss = 0.0\n","        train_disc_loss = 0.0\n","        train_enc_loss = 0.0\n","        \n","        test_reconst_loss = 0.0\n","        test_disc_loss = 0.0\n","        test_enc_loss = 0.0\n","\n","        for batch in train_dataloader:\n","            imgs, labels = batch\n","            imgs = imgs.view(imgs.shape[0], -1).to(device)\n","            labels = labels.to(device)\n","            \n","            enc.zero_grad()\n","            dec.zero_grad()\n","            disc.zero_grad()\n","             \n","            encoded = enc(imgs)\n","            decoded = dec(encoded)\n","            \n","            reconstruction_loss = F.binary_cross_entropy(decoded, imgs)\n","            reconstruction_loss.backward()\n","            \n","            optim_enc.step()\n","            optim_dec.step()\n","            enc.eval()\n","\n","            latent_dim, disc_inter_dim = disc.get_dims()\n","            real = torch.randn(imgs.shape[0], latent_dim).to(device)\n","            \n","            disc_real = disc(real)\n","            disc_fake = disc(enc(imgs))\n","            \n","            disc_loss = -torch.mean(torch.log(disc_real + eps) + torch.log(1.0 - disc_fake + eps))\n","            disc_loss.backward()\n","            \n","            optim_dec.step()\n","            enc.train()\n","            \n","            disc_fake = disc(enc(imgs))\n","            enc_loss = -torch.mean(torch.log(disc_fake + eps))\n","            enc_loss.backward()\n","            \n","            optim_enc_reg.step()\n","\n","            train_reconst_loss += reconstruction_loss.item()\n","            train_disc_loss += disc_loss.item()\n","            train_enc_loss += enc_loss.item()\n","\n","        enc.eval()\n","        dec.eval()\n","        disc.eval()\n","\n","        with torch.no_grad():\n","            for batch in test_dataloader:\n","                imgs, labels = batch\n","                imgs = imgs.view(imgs.shape[0], -1).to(device)\n","                labels = labels.to(device)\n","\n","                encoded = enc(imgs)\n","                decoded = dec(encoded)\n","\n","                reconstruction_loss = F.binary_cross_entropy(decoded, imgs)\n","\n","                latent_dim, disc_inter_dim = disc.get_dims()\n","                real = torch.randn(imgs.shape[0], latent_dim).to(device)\n","\n","                disc_real = disc(real)\n","                disc_fake = disc(enc(imgs))\n","                disc_loss = -torch.mean(torch.log(disc_real + eps) + torch.log(1.0 - disc_fake + eps))\n","\n","                disc_fake = disc(enc(imgs))\n","                enc_loss = -torch.mean(torch.log(disc_fake + eps))\n","\n","                test_reconst_loss += reconstruction_loss.item()\n","                test_disc_loss += disc_loss.item()\n","                test_enc_loss += enc_loss.item()\n","\n","        train_reconst_loss /= len(train_dataloader)\n","        train_disc_loss /= len(train_dataloader)\n","        train_enc_loss /= len(train_dataloader)\n","        \n","        test_reconst_loss /= len(test_dataloader)\n","        test_disc_loss /= len(test_dataloader)\n","        test_enc_loss /= len(test_dataloader)\n","\n","        tqdm_dct = {'train reconst loss:': train_reconst_loss, 'train disc loss:': train_disc_loss, 'train enc loss': train_enc_loss, \\\n","                        'test reconst loss:': test_reconst_loss, 'test disc loss:': test_disc_loss, 'test enc loss': test_enc_loss}\n","        tqdm_iter.set_postfix(tqdm_dct, refresh=True)\n","        tqdm_iter.refresh()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.578419Z","iopub.status.busy":"2022-04-08T00:43:26.577933Z","iopub.status.idle":"2022-04-08T00:43:26.589197Z","shell.execute_reply":"2022-04-08T00:43:26.588532Z","shell.execute_reply.started":"2022-04-08T00:43:26.578373Z"},"trusted":true},"outputs":[],"source":["models = (aae_encoder, aae_decoder, aae_discriminator)\n","optimizers = (optim_encoder, optim_encoder_regularization, optim_decoder, optim_discriminator)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:43:26.592138Z","iopub.status.busy":"2022-04-08T00:43:26.590197Z","iopub.status.idle":"2022-04-08T00:52:49.002321Z","shell.execute_reply":"2022-04-08T00:52:49.001592Z","shell.execute_reply.started":"2022-04-08T00:43:26.592099Z"},"trusted":true},"outputs":[],"source":["train_aae(dataloaders, models, optimizers, epochs, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-08T00:52:49.004318Z","iopub.status.busy":"2022-04-08T00:52:49.003842Z","iopub.status.idle":"2022-04-08T00:52:50.737055Z","shell.execute_reply":"2022-04-08T00:52:50.736379Z","shell.execute_reply.started":"2022-04-08T00:52:49.004280Z"},"trusted":true},"outputs":[],"source":["aae_encoder.eval()\n","aae_decoder.eval()\n","predictions = []\n","plots = 10\n","for i, data in enumerate(test_dataset):\n","    if i == plots:\n","        break\n","    pred = aae_decoder(aae_encoder(data[0].to(device).unsqueeze(0).view(1, 784)))\n","    predictions.append(pred.view(1, 28, 28).detach().cpu())\n","plotn(plots, test_dataset)\n","plotn(plots, predictions)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
